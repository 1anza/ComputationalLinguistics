{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7dwyEKcCGpx"
      },
      "source": [
        "# A Quick Introduction to Word2Vec with Python and Gensim\n",
        "April 7, 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPKosWFjCGp3"
      },
      "source": [
        "## Setting up and getting some data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XMULWTxCGp4"
      },
      "source": [
        "# We need a resource for our data\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8am5-PssCGp5"
      },
      "source": [
        "# If you are using this in CoLab, also run this cell, otherwise you can skip it\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1_jRsY-CGp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de72d7c2-186e-4189-b16c-f001e154f2f8"
      },
      "source": [
        "# We are going to use the brown corpus\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bg2Gf9byCGp6"
      },
      "source": [
        "Let's start by printing a few sentences out of the \"brown\" corpus, to get an idea of what the data looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lxxHqRrCGp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690344ee-a8b9-45ec-d94a-9d8ebbfde1af"
      },
      "source": [
        "brown_sent = brown.sents()\n",
        "print(brown_sent[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yyZ3Y0XCGp6"
      },
      "source": [
        "## Building the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H__ioQy3CGp6"
      },
      "source": [
        "We don't want to build the whole model from scratch, we will use the Gensim library instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPyvNxluCGp7"
      },
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaSwiWIyCGp7"
      },
      "source": [
        "We can now build an instance of the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjF0VMYMCGp7"
      },
      "source": [
        "# This is the whole model for the brown corpus (it might take a few minutes)!\n",
        "brown_model = Word2Vec(brown_sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byGADIbKCGp7"
      },
      "source": [
        "Let's look at an example!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0B20Qi5CGp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f07860-c097-4388-aa0f-74d0021c6a4a"
      },
      "source": [
        "test1 = brown_model.wv.most_similar('blue')\n",
        "print(\"Most similar to 'blue':\\n\", test1[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar to 'blue':\n",
            " [('red', 0.9627671241760254), ('gray', 0.9619156718254089), ('green', 0.9587184190750122)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6eBbaerCGp9"
      },
      "source": [
        "## Refining the model\n",
        "\n",
        "Word2Vec takes a broad range of parameters. In our example above, we only chose where to get our sentences from, and we used the *default* settings for the rest. But let's now look at a few that are most relevant (you can find a full list here: https://radimrehurek.com/gensim/models/word2vec.html):\n",
        "\n",
        "- **size**: The dimensionality of our embeddings (i.e. the length of each word vector).\n",
        "- **window**: Which words are considered contexts of the target. The size of window affects the type of similarity captured in the embeddings.\n",
        "- **negative**: The number of negative samples (incorrect training-pair instances) that are drawn for each good.\n",
        "- **sg**: Training algorithm -- 1 for skip-gram; otherwise CBOW.\n",
        "- **min_count**: Ignores all words with total frequency lower than this.\n",
        "- **iter**: Number of iterations (epochs) over the corpus.\n",
        "\n",
        "So let's now train our model by explicitly setting some of these parameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGX3o-wTCGp9"
      },
      "source": [
        "# This is the whole model (it's going to take a few minutes!)\n",
        "brown_model = Word2Vec(brown_sent, size = 300, window = 5, negative = 5, sg = 1, min_count = 5, iter = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwNbGX61CGp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9240377-9e2a-42a3-8961-e32d09e42e94"
      },
      "source": [
        "# We can do the same test as before\n",
        "test = brown_model.wv.most_similar('game')\n",
        "print(\"Most similar to 'blue':\\n\", test[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar to 'blue':\n",
            " [('golf', 0.7113315463066101), ('sunny', 0.6928216218948364), ('baseball', 0.6793524026870728), ('Beethoven', 0.6783057451248169), (\"week's\", 0.6782501339912415), ('tournament', 0.6765882968902588), ('games', 0.674275279045105), ('Bears', 0.673066258430481), ('booking', 0.6693858504295349), ('orchestra', 0.6671175956726074)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx9JWuqlCGp9"
      },
      "source": [
        "## Evaluating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asLIKZ2nCGp-"
      },
      "source": [
        "We are going to rely on our own **human intuitions** to decide how well the model is doing!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCsUj8MHCGp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6647e2-8aeb-4c21-d77c-8c276895d588"
      },
      "source": [
        "sim = brown_model.wv.similarity(\"cup\", \"water\")\n",
        "print(\"How similar is 'cup' to 'water':\\n\", sim)\n",
        "\n",
        "sim = brown_model.wv.similarity(\"cup\", \"book\")\n",
        "print(\"How similar is 'cup' to 'book':\\n\", sim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How similar is 'cup' to 'water':\n",
            " 0.5959407\n",
            "How similar is 'cup' to 'book':\n",
            " 0.18765117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT4bgd-HCGp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "355edb69-60d7-4ad9-bf55-eb2aed278823"
      },
      "source": [
        "brown_test = brown_model.wv.most_similar('child')\n",
        "print(\"Most similar to 'child':\\n\", brown_test[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar to 'child':\n",
            " [('teacher', 0.6705455780029297), ('autistic', 0.6585035920143127), ('parent', 0.6541589498519897)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfTS2eGgCGp-"
      },
      "source": [
        "We can do more complex comparisons, but some results will be less intuitive than others!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gfevG4FCGp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c781f572-b938-4d06-93ea-992f92bcf2c4"
      },
      "source": [
        "brown_test = brown_model.wv.most_similar(positive = ['child'], negative = ['person'])\n",
        "print(\"Most similar to 'child' but dissimilar to 'person':\\n\", brown_test[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar to 'child' but dissimilar to 'person':\n",
            " [('health', 0.3028700649738312), ('children', 0.25111696124076843), ('fever', 0.21571321785449982)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OepVBfzvCGp-"
      },
      "source": [
        "### Let's try a few more interesting tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLlaRH0fCGp-"
      },
      "source": [
        "Which word is a mismatch in the sequence?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoGzI77cCGp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb147427-ed70-487f-a685-53573ec90605"
      },
      "source": [
        "mismatch = brown_model.wv.doesnt_match(['teacher','professor','doctor','red','athlete','runner'])\n",
        "print(mismatch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "red\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NadQXuRhCGp_"
      },
      "source": [
        "Maybe not **just** semantic relations?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4AKRwb7CGp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2704da1-1ada-4144-a435-12c837f17064"
      },
      "source": [
        "mismatch = brown_model.wv.doesnt_match(['running','swimming','singing','paper','reading','booking','catch'])\n",
        "print(mismatch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx54ozS6CGp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be5664e-ccc0-4980-9ca8-b9b6039e0a9e"
      },
      "source": [
        "compare = brown_model.wv.similarity('walk','walked') \n",
        "print(\"The similarity between 'walk' and 'walked':\\n\", compare)\n",
        "\n",
        "compare = brown_model.wv.similarity('look','looked') \n",
        "print(\"The similarity between 'look' and 'looked':\\n\", compare)\n",
        "\n",
        "compare = brown_model.wv.similarity('look','walk') \n",
        "print(\"The similarity between 'look' and 'walk':\\n\", compare)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity between 'walk' and 'walked':\n",
            " 0.6559779\n",
            "The similarity between 'look' and 'looked':\n",
            " 0.6430508\n",
            "The similarity between 'look' and 'walk':\n",
            " 0.527116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xaq0BKT7CGp_"
      },
      "source": [
        "## The choice of training data\n",
        "\n",
        "As for the other parameters that we looked at, the **choice of training data** (our corpus) is essential in driving model performance.\n",
        "For example, consider a very famous test case for Word2Vec: is the model able to derive the fact that \"woman\" is to \"queen\" what \"man\" is to \"king\"?\n",
        "\n",
        "We can represent this question algebraically as:\n",
        "\n",
        "$$vector(woman) +  vector(king) - vector(man) = vector(queen)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY0Zny_TCGp_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4fc8bc6-e3b6-40a3-824d-636fb3efafdd"
      },
      "source": [
        "test = brown_model.wv.most_similar(positive=['woman','king'], negative=['man'], topn=1)\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('singing', 0.72732013463974)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVPENvc9CGp_"
      },
      "source": [
        "We got a *weird* result!\n",
        "\n",
        "However, consider the fact that the brown corpus is not too big (1M words) and it is fairly old. What would happen if we used a bigger, more recent corpus?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8qLaCJ_CGp_"
      },
      "source": [
        "### Working with a pretrained model\n",
        "\n",
        "Luckily, NLTK includes a pre-trained model. In particular, it includes part of a model trained on 100 billion words from the Google News Dataset. The full model is from https://code.google.com/p/word2vec/ (about 3 GB)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPwOoc1UCGqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e502de-26c8-4253-f70e-63b8bd171f15"
      },
      "source": [
        "# we need to get the data\n",
        "from nltk.data import find\n",
        "nltk.download('word2vec_sample')\n",
        "\n",
        "# we are going to use a pruned set\n",
        "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package word2vec_sample to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/word2vec_sample.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlCWX7fUCGqA"
      },
      "source": [
        "# This time we are **not** training it from scratch, we are just loading it in (it is still going to take a bit)!\n",
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3BtG6NyCGqA"
      },
      "source": [
        "Let's do a sanity check!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkz91uJgCGqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4281a26f-afe0-4003-f74a-e1e9dcb3e3d9"
      },
      "source": [
        "model.most_similar(\"joy\")[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('elation', 0.6732936501502991),\n",
              " ('joyful', 0.6633968353271484),\n",
              " ('delight', 0.655024528503418),\n",
              " ('excitement', 0.6531193256378174),\n",
              " ('thrill', 0.630203902721405),\n",
              " ('happiness', 0.6182849407196045),\n",
              " ('joyous', 0.6128898859024048),\n",
              " ('jubilation', 0.6043694615364075),\n",
              " ('pleasure', 0.6032876968383789),\n",
              " ('sadness', 0.5980007648468018)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD_zPjcNCGqA"
      },
      "source": [
        "Let's try our example once more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aybydK8CGqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa05d7b2-2648-4957-a46e-199bb860d1e5"
      },
      "source": [
        "model.most_similar(positive=['woman','king'], negative=['man'], topn = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7118192911148071)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P044jWvCGqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3322051e-4443-4d10-b464-40eb8af67aad"
      },
      "source": [
        "model.most_similar(positive=['Paris','Germany'], negative=['Berlin'], topn = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('France', 0.7884092330932617)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXPghiAHCGqA"
      },
      "source": [
        "We can do more! Let's track **semantic shifts** (e.g. historical changes in meaning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90HWMgI8CGqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce1ae19-5c4b-44e5-edd4-f773bbd03c4b"
      },
      "source": [
        "change1 = brown_model.wv.most_similar('gay')\n",
        "print(\"Most similar to 'gay' in the brown corpus:\\n\", change1[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar to 'gay' in the brown corpus:\n",
            " [('gaiety', 0.8070326447486877), ('sad', 0.8015825748443604), ('wonderfully', 0.7922199964523315), ('Schwarzkopf', 0.7886055707931519), ('witty', 0.7874990701675415)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBUamVUlCGqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb6bb65-c47a-4c54-90be-9c3b76d27b83"
      },
      "source": [
        "change2 = model.most_similar('gay')\n",
        "print(\"Most similar to 'gay' in Google News:\\n\", change2[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar to 'gay' in Google News:\n",
            " [('homosexual', 0.8145634531974792), ('homosexuals', 0.7562745809555054), ('lesbians', 0.7516927719116211), ('queer', 0.6972684264183044), ('Gay', 0.6740463376045227)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GcRh-pMCGqB"
      },
      "source": [
        "## Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7u6pqtXCGqB"
      },
      "source": [
        "Relying on frequency patterns in human-generated data to make inferences has some problems..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEwG3-5jCGqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff114e08-dd73-47ef-88ea-632c5bbb3cdf"
      },
      "source": [
        "compare1 = model.similarity('she','engineer')\n",
        "print(\"The similarity between 'she' and 'engineer':\\n\", compare1)\n",
        "\n",
        "compare2 = model.similarity('he','engineer')\n",
        "print(\"The similarity between 'he' and 'engineer':\\n\", compare2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity between 'she' and 'engineer':\n",
            " 0.0032564793\n",
            "The similarity between 'he' and 'engineer':\n",
            " 0.107617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUBMQ1AQCGqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54da939c-07f0-4cf7-9689-65db0e18da1d"
      },
      "source": [
        "compare1 = model.similarity('woman','nurse')\n",
        "print(\"The similarity between 'woman' and 'nurse':\\n\", compare1)\n",
        "\n",
        "compare2 = model.similarity('man','nurse')\n",
        "print(\"The similarity between 'man' and 'nurse':\\n\", compare2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity between 'woman' and 'nurse':\n",
            " 0.44135568\n",
            "The similarity between 'man' and 'nurse':\n",
            " 0.25472283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONr5nwU8CGqC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKPK5TQndLMH"
      },
      "source": [
        "**Exercise 1. (5 points)**\n",
        "\n",
        "Pick 2 words of your choice as use the code below to extract their 3 closest words from the brown semantic space and the google semantic space. Which model better captures your intuition? Sum up your considerations in a few sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EddjfpYadm05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b0ae1b6-6e63-465b-c85e-3ed7bdcb1785"
      },
      "source": [
        "# Word 1\n",
        "# Brown modify this code with a word of your choice\n",
        "\n",
        "brown_test = brown_model.wv.most_similar('lazy')\n",
        "print(\"Brown Corpus, most similar to 'w1':\\n\", brown_test[:4])\n",
        "\n",
        "#Google modify this code with words of your choice\n",
        "\n",
        "google_test = model.most_similar(\"lazy\")\n",
        "print(\"Google Corpus, most similar to 'w1':\\n\", google_test[:4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brown Corpus, most similar to 'w1':\n",
            " [('redhead', 0.878506064414978), ('Sabella', 0.8738782405853271), ('selfish', 0.8700441122055054), ('lukewarm', 0.8653857707977295)]\n",
            "Google Corpus, most similar to 'w1':\n",
            " [('slothful', 0.5775077939033508), ('stupid', 0.5557514429092407), ('dumb', 0.5406800508499146), ('tired', 0.5291409492492676)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2SYPG3kdvY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7639a3-c451-44cb-f7f1-5ea0ac172784"
      },
      "source": [
        "# Word 2\n",
        "# Brown modify this code with a word of your choice\n",
        "\n",
        "brown_test = brown_model.wv.most_similar('baby')\n",
        "print(\"Brown Corpus, most similar to 'w2':\\n\", brown_test[:4])\n",
        "\n",
        "#Google modify this code with words of your choice\n",
        "\n",
        "google_test = model.most_similar(\"baby\")\n",
        "print(\"Google Corpus, most similar to 'w2':\\n\", google_test[:4])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brown Corpus, most similar to 'w2':\n",
            " [('cook', 0.7868125438690186), ('uncle', 0.7821295857429504), ('Carla', 0.778669536113739), ('Grandma', 0.7778461575508118)]\n",
            "Google Corpus, most similar to 'w2':\n",
            " [('newborn', 0.8206996917724609), ('babies', 0.7815852165222168), ('infant', 0.7726625800132751), ('child', 0.65739905834198)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUdfOH9VefJT"
      },
      "source": [
        "*Well, the Google Corpus tends to fit the bill rather well in each instance - definitely following my intuition. Brown gave reliably strange comparisons for both 'lazy' and 'baby', hilariously finding 'redhead' to be the most similar to lazy and 'cook' as most similar to baby. The 60s were a strange time indeed.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2Cn2L7ucEbt"
      },
      "source": [
        "**Exercise 2. (5 points)**\n",
        "\n",
        "Think of two more cases of implicit biases that you can test the model on (they can be based on gender as above, but it would be even better if you could think of other dimensions for bias). Then, modify the code below by switching w1 and w2 with words of your choice to test your idea. Did the model output what you expected? Summarize your conclusions in a couple of sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOlTm0-ecsGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1975383-64c0-4d38-901b-3a0ab7807c5f"
      },
      "source": [
        "#Bias example 1\n",
        "\n",
        "compare3 = model.similarity('virus', 'computer')\n",
        "print(\"The similarity between virus and computer:\\n\", compare3)\n",
        "\n",
        "compare3s = model.similarity('virus','cell')\n",
        "print(\"The similarity between virus and cell:\\n\", compare3s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity between virus and computer:\n",
            " 0.27622563\n",
            "The similarity between virus and cell:\n",
            " 0.20255555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLVnhUXucwsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1b90b1-f97c-401c-ae60-37d1b37437b0"
      },
      "source": [
        "#Bias example 2\n",
        "\n",
        "compare4 =  model.similarity('American', 'fear')\n",
        "print(\"The similarity between American and fear:\\n\", compare4)\n",
        "\n",
        "compare4s = model.similarity('Arab','fear')\n",
        "print(\"The similarity between Arab and fear:\\n\", compare4s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The similarity between American and fear:\n",
            " 0.07104741\n",
            "The similarity between Arab and fear:\n",
            " 0.15490128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y--pfAxZcs3P"
      },
      "source": [
        "#Write your consideration here\n",
        "\n",
        "There is some interesting bias present in this corpus.\n",
        "\n",
        "When testing for two different 'virus' hosts, the corpus prefers associating a virus to a computer rather than a cell. In reality, the term 'virus' should be equally similar regardless of host type.\n",
        "\n",
        "This one was somewhat expected, but there is a bias towards the word 'Arab' and the word fear. When using a different demographic like 'American', the fear association cuts in half. This bias is likely due to potential news articles on terrorism in the Google corpus. "
      ]
    }
  ]
}