{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccD5Kq24WAN3"
      },
      "source": [
        "<div style=\"text-align: right\">\n",
        "    <i>\n",
        "        Spring 2022 <br>\n",
        "    </i>\n",
        "</div>\n",
        "\n",
        "# Preliminaries (2 points)\n",
        "\n",
        "1. Load in all of the packages you will need for this assignment in the cell below. \n",
        "\n",
        "  If you load in other packages later in the notebook, be sure to bring them up here. This is good coding practice and will look cleaner for everyone when reading your code.\n",
        "\n",
        "  You will need the following:\n",
        "\n",
        "  * To load a plain text file in with the colab interface (by uploading the file to the notebook)\n",
        "  * To count occurrances of tokens\n",
        "  * A regular expression tokenizer\n",
        "  * The NLTK tokenizer for English\n",
        "  * The spaCy word tokenizer for English\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfOQBQztFnqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95cb9918-449a-4335-e832-fc6aa224ccc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Load in packages that you will use in this notebook\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "# put other packages you will use below this line\n",
        "from google.colab import files\n",
        "from collections import Counter  \n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import spacy\n",
        "import sys\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZSPSo-TWTxL"
      },
      "source": [
        "# Preliminaries 2 (1 point)\n",
        "\n",
        "Load in the file called `faustus.txt` in the variable `faust`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl225PcgV61a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "0d1aed7c-cfc9-49c1-a9aa-4c30eb62a3a7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ce133327-35d0-4334-b0e5-44e423d4be22\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ce133327-35d0-4334-b0e5-44e423d4be22\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving faustus_clean.txt to faustus_clean.txt\n"
          ]
        }
      ],
      "source": [
        "#Import the file here, as done in class\n",
        "faustus = files.upload()\n",
        "faustus_full  = faustus['faustus_clean.txt'].decode('utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRL5OkT4Wn-4"
      },
      "source": [
        "# Question 1: 2 points\n",
        "\n",
        "In this section, we will be comparing different preprocessing strategies. \n",
        "\n",
        "First, tokenize the corpus using a simple regEx technique that relies on whitespaces to distinguish words (You can use the function we defined in class)!\n",
        "\n",
        "Save the resulting list into the variable `faust_regEx`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq6-78b1IGPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1b1dd1-db54-4b4c-df69-0b6816624388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not marching in the fields of thrasymene where mars did\n"
          ]
        }
      ],
      "source": [
        "# define a tokenize function using regex based on whitespace\n",
        "def tokenize(the_string):\n",
        "    \"\"\"Convert string to list of words\"\"\"\n",
        "    return re.findall(r\"\\w+\", the_string)\n",
        "\n",
        "\n",
        "# define a variable for each token list\n",
        "faust_regEx = tokenize(faustus_full)\n",
        "\n",
        "# print the first 10 elements as a safety check\n",
        "print(*faust_regEx[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UqHOFeVIGzB"
      },
      "source": [
        "# Question 2: 5 points\n",
        "\n",
        "Now, we are going to use the [`nltk` `word_tokenize`](https://www.kite.com/python/docs/nltk.word_tokenizefunction). You should have loaded this above in the very first block. Use `word_tokenize` on the corpus in a list of string arrays called `faust_nltk_tokenized`. Use a slice to print the fifth to the tenth elements of the array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8oOzoHiIzdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ea1ed2-738c-43cf-e1e8-9b23e4c62968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not marching in the fields of thrasymene , where mars\n"
          ]
        }
      ],
      "source": [
        "# use nltk's word_tokenize function and save the output into a variable\n",
        "faust_nltk_tokenized = nltk.word_tokenize(faustus_full)\n",
        "\n",
        "# print the first 10 elements as a safety check\n",
        "print(*faust_nltk_tokenized[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PpfhdWSI0Me"
      },
      "source": [
        "# Question 3: 5 points\n",
        "\n",
        "Now, we are going to use the [`spacy`](https://spacy.io/usage/spacy-101) tokenization [function](https://spacy.io/api/tokenizer). \n",
        "\n",
        "The output that spacy gives you is more complicated than the output of `nltk`'s `word_tokenize` function, because the `spacy` API takes a string (e.g., \"I like cheese\") and returns a `doc` object. Within the `doc` object there are `token`s, and each `token` has a `text` object. \n",
        "\n",
        "For this question, what you need to do is:\n",
        "\n",
        "* load the spacy tokenizer with \"en\" as the default value\n",
        "* obtain the `doc` object by using the tokenizer over the list `faust_full` \n",
        "* instantiate an empty list `faust_spacy_tokenized` and implement a for loop through all of the  `token`s of the obtained doc, storing for each the `token.text` string object into your list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTwCwQ7nM8dX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d787329-ad64-40a1-acbf-f5e1fb007aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not marching in the fields of thrasymene , \n",
            " where\n"
          ]
        }
      ],
      "source": [
        "# use spacy's tokenization features\n",
        "# You must import spacy at the beginning of the notebook\n",
        "\n",
        "# load spacy with en as the vocabulary\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# get the doc object for the faust corpus\n",
        "doc = nlp(faustus_full)\n",
        "\n",
        "# save the output into a variable\n",
        "faust_spacy_tokenized = [token.text for token in doc]\n",
        "\n",
        "# print the first 10 elements as a safety check\n",
        "print(*faust_spacy_tokenized[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLC2daI-Myn1"
      },
      "source": [
        "# Question 4: Compare tokenizations (5 points)\n",
        "\n",
        "Now that we have three tokenizations, we want to compare how similar the tokenizations are. \n",
        "\n",
        "In the code cell below, Visualize a reasonaly large subset of the corpus, tokenized under each of the three approaches above.\n",
        "Then, in the cell below that, explain how the outputs of these tokenizations differ, and what you can infer from the output about the ways the tokenization techniques differ (If necessary, you can modify the number of tokens you visualize until you converge on a good sample for the comparison). \n",
        "\n",
        "What do you think are the strengths and weaknesses of each tokenization approach? Do you think one of the tokenizations is better than another? Can you think of a way you would test which one is better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sZ7km6oRYZs"
      },
      "source": [
        "### Question 4A: Code (2/5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld_dCKjLQaqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1568ebdc-62c8-4f7a-fca8-a23f6d42a6df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not marching in the fields of thrasymene where mars did mate the warlike carthagens nor sporting in the dalliance of love in courts of kings where state is overturn d\n",
            "\n",
            "not marching in the fields of thrasymene , where mars did mate the warlike carthagens ; nor sporting in the dalliance of love , in courts of kings where state\n",
            "\n",
            "not marching in the fields of thrasymene , \n",
            " where mars did mate the warlike carthagens ; \n",
            " nor sporting in the dalliance of love , \n",
            " in courts of\n"
          ]
        }
      ],
      "source": [
        "# Put your code here\n",
        "print(*faust_regEx[:30])\n",
        "print()\n",
        "print(*faust_nltk_tokenized[:30])\n",
        "print()\n",
        "print(*faust_spacy_tokenized[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CbwIX8AQm9Y"
      },
      "source": [
        "### Question 4B: Free response (3/5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCVSKvDZGUDS"
      },
      "source": [
        "$\\color{red}{\\text{Of the three tokenizers regEx remains the most rudimentary, only accounting for words without consideration for punctuation.}}$\n",
        "\n",
        "$\\color{red}{\\text{Nltk's tokenizer accomodates the punctuation issues of our simple regex.}}$\n",
        "\n",
        "$\\color{red}{\\text{SpaCy's tokenizer takes this a step further by also including a newline whenever a punctuation point is reached.}}$\n",
        "\n",
        "I would personally use either nltk or spacy depending on what I was looking to accomplish. I can see how the newlines added by spacy would be nice to have, since they provide clear terminators for each sentence/phrase. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlivQPJXf6m2"
      },
      "source": [
        "# Question 5: Tabulating word counts under different algorithms (10 points)\n",
        "\n",
        "Now that you have compared and contrasted different tokenization algorithms, consider the effect that tokenization can have on our ability to characterize a corpus as a whole. \n",
        "\n",
        "Load in the `Counter` module and extract counts of all of the words under each of the three tokenizations schemes. Look at the top 5 most frequent (using the `.most_frequent()` method) and the top 10 least frequent (hint: use negative indices) words. In our data, what appear to be the biggest sources of disagreement? Do these confirm or disconfirm your hypotheses in the previous question? How or how not? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjwPTSJA_txp"
      },
      "source": [
        "### Question 5A: Code (5/10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiXEsEaVdJTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7837ba5-c511-4e73-d5f0-de8f7e98a710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('and', 509) ('the', 501) ('i', 400) ('of', 338) ('to', 333)\n",
            "\n",
            "(',', 1962) ('.', 562) ('and', 504) ('the', 500) ('i', 396)\n",
            "\n",
            "('\\n', 1972) (',', 1953) ('.', 560) ('and', 507) ('the', 500)\n",
            "\n",
            "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
            "\n",
            "('exhort', 1) ('unlawful', 1) ('deepness', 1) ('entice', 1) ('practise', 1) ('permits', 1) ('hora', 1) ('diem', 1) ('auctor', 1) ('opus', 1)\n",
            "\n",
            "('exhort', 1) ('unlawful', 1) ('deepness', 1) ('entice', 1) ('practise', 1) ('permits', 1) ('hora', 1) ('diem', 1) ('auctor', 1) ('opus', 1)\n",
            "\n",
            "('exhort', 1) ('unlawful', 1) ('deepness', 1) ('entice', 1) ('practise', 1) ('permits', 1) ('hora', 1) ('diem', 1) ('auctor', 1) ('opus', 1)\n"
          ]
        }
      ],
      "source": [
        "## Your code for question 5A\n",
        "regExCounter = Counter(faust_regEx)\n",
        "nltkCounter = Counter(faust_nltk_tokenized)\n",
        "spacyCounter = Counter(faust_spacy_tokenized)\n",
        "\n",
        "print(*regExCounter.most_common(5))\n",
        "print()\n",
        "print(*nltkCounter.most_common(5))\n",
        "print()\n",
        "print(*spacyCounter.most_common(5))\n",
        "\n",
        "print(\"\\n\" + \"_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\" + \"\\n\")\n",
        "\n",
        "print(*regExCounter.most_common()[-10:])\n",
        "print()\n",
        "print(*nltkCounter.most_common()[-10:])\n",
        "print()\n",
        "print(*spacyCounter.most_common()[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bud1U9iM_vgh"
      },
      "source": [
        "### Question 5B: Free response (5/10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTWirBBEGPjO"
      },
      "source": [
        "It's about what you'd expect, with nltk and spacy consisting of the additional punctuation symbols, while spacy exclusively adds the adidtional newlines. Once you get beyond the symbols, the relevant word counts remain similarly useful across the board."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FvuYyre5F_P"
      },
      "source": [
        "# Question 6: Tabulating pointwise mutual information (15 points)\n",
        "\n",
        "Mutual information is a computation that is very similar to computing a conditional probability. Recall that computing a conditional probability, defined below, requires knowing two probabilities. The first, $p(A \\cap B)$, is the probability of observing $A$ and $B$ at the same time (so observing the bigram as a type). The second, $p(A)$, is the probability of observing $A$ across all contexts.\n",
        "\n",
        "Recall that we have used MLE so to approximate all of these by their frequencies in a corpus. For example, $p(A)$ can be approximated by:\n",
        "\n",
        "<center> $\\large p(A) \\approx \\frac{count(A)}{\\sum_{w \\in V}count(w)}$ </center>\n",
        "\n",
        "Mutual information is very similar, but requires dividing the co-occurence statistic by two probabilities $p(A)$ and $p(B)$.\n",
        "\n",
        "\n",
        "<center>$\\large MI = \\frac{p(A \\cap B)}{p(A) \\cdot p(B)}$</center>\n",
        "\n",
        "<hr />\n",
        "\n",
        "\n",
        "This question contains multiple parts to respond to.\n",
        "\n",
        "1. Compute the bigram frequencies of all words in our `faustus` corpus. You may use whatever tokenization scheme you think performs the best.\n",
        "2. For each of the bigrams in that abstracts, compute the mutual information of that bigram\n",
        "3. Pick a subset of bigrams and and print their mutual information value to the notebook.\n",
        "4. Answer the questions in the free response section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pXgmepfDDAm"
      },
      "source": [
        "### Question 6A: Computing mutual information for bigrams in one sentence (10/15 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXPXWtAfCRGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f02bb799-206d-40da-ff94-b8170fe4fadb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('0', 'what') 7.087920018211499\n",
            "('a', 'band') 6.2179805587758725\n",
            "('a', 'banquet') 5.2179805587758725\n",
            "('a', 'bill') 4.633018058054716\n",
            "('a', 'book') 3.410625636718268\n",
            "('a', 'bottle') 6.2179805587758725\n",
            "('a', 'bridge') 4.633018058054716\n",
            "('a', 'bundle') 6.2179805587758725\n",
            "('a', 'cardinal') 6.2179805587758725\n",
            "('a', 'castle') 5.633018058054716\n"
          ]
        }
      ],
      "source": [
        "def count_unigrams(input_list):\n",
        "    frequency = {}\n",
        "    for word in input_list:\n",
        "        if word in frequency:\n",
        "            frequency[word] += 1\n",
        "        else:\n",
        "            frequency[word] = 1\n",
        "    return frequency\n",
        "\n",
        "def count_bigrams(input_list):\n",
        "    bigrams = [tuple(input_list[inx:inx + 2])\n",
        "               for inx in range(len(input_list) - 1)]\n",
        "\n",
        "    frequency_bigrams = {}\n",
        "    for bigram in bigrams:\n",
        "        if bigram in frequency_bigrams:\n",
        "            frequency_bigrams[bigram] += 1\n",
        "        else:\n",
        "            frequency_bigrams[bigram] = 1\n",
        "    return frequency_bigrams\n",
        "\n",
        "def mutual_info(input_list, freq_unigrams, freq_bigrams):\n",
        "    mi = {}\n",
        "    factor = len(input_list) * len(input_list) / (len(input_list) - 1)\n",
        "    for bigram in freq_bigrams:\n",
        "        mi[bigram] = (\n",
        "            math.log(factor * freq_bigrams[bigram] /\n",
        "                     (freq_unigrams[bigram[0]] *\n",
        "                      freq_unigrams[bigram[1]]), 2))\n",
        "    return mi\n",
        "\n",
        "\n",
        "mutual_info_values = mutual_info(faust_regEx, count_unigrams(faust_regEx), count_bigrams(faust_regEx))\n",
        "\n",
        "for key in sorted(mutual_info_values)[:10]:\n",
        "  print (key, mutual_info_values[key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIEw6FcsdZ1f"
      },
      "source": [
        "### Question 6B: Free response (5/10 points)\n",
        "\n",
        "\n",
        "The values seen at the head of our dictionary are all positive, which tells me that the bigrams are occuring together with a higher likelihood than they are as individual unigrams (hopefully I'm understanding that correctly??). \n",
        "\n",
        "Playing around with this a bit more, it does look like some mutual_info values are negatve, so I'd assume that means those bigrams are less likely to occur compared with their unigrams?\n",
        "\n",
        "As for which is 'better', I'm leaning on mutual information for the simple fact that it better accounts for the probabilities of occurrence of each unigram in the bigram relative to the entire corpus. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCgDQrOaICRl"
      },
      "source": [
        "# Self Evaluation (5 points, added only if at least half of the previous exercises has been attempted)\n",
        "\n",
        "\n",
        "Overall this was relatively easy considering the vast amount of resources out there. As lame as this is, getting the first 10 dictionary entries to print was a bit tricky, but got there in the end. Enjoyed seeing the different tokenization methods - ended up just using the regEx since we weren't getting super analytical, but it would be interesting to incorporate the spaCy protocol in the future.  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}